{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Benchmark - Wind-dependent Variables: Predict Wind Speeds of Tropical Storms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure><center><img src=\"https://drivendata-public-assets.s3.amazonaws.com/re-hurricane-irma.jpg\" width=\"500\" height=\"40\"></center></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><center><i>Hurricane Irma Striking Miami, Florida. Credit: Warren Faidley, Getty Images</i></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the benchmark solution tutorial for our brand new [competition](https://www.drivendata.org/competitions/72/predict-wind-speeds/) run in partnership with Radiant Earth Foundation and the [NASA Interagency Implementation and Advanced Concepts (IMPACT)](https://earthdata.nasa.gov/esds/impact) team.\n",
    "\n",
    "In this challenge, you are tasked with estimating the wind speeds of tropical storms at different points in time using satellite images captured throughout a storm's life cycle. Because storm damage models rely on wind speed to approximate risk, accurate estimates can directly improve short-term storm intensity forecasting. Your machine learning models and insights have the potential to meaningfully strengthen disaster preparedness efforts around the world.\n",
    "\n",
    "The training data for this competition consist of over 70,000 single-band images from storms in the Atlantic and East Pacific Oceans. The data come from a satellite band representing long-wave infrared frequency, which increases the brightness of clouds and should help us to better capture the spatial structure of storms. Each image has a corresponding wind speed contained in a separate file, measured in knots. For each image in the test set, you will predict the corresponding wind speed and will submit your predictions based on the `submission_format.csv` provided.\n",
    "\n",
    "**In this post, we will show you how to load, explore, and prepare the data, as well as implement a first-pass deep learning model for predicting wind speeds.**\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand what's in our data, we can begin by exploring trends across oceans, storms, wind speeds, and relative time using the training metadata.\n",
    "\n",
    "Let's load the metadata and look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"from pathlib import Path\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport os\\nimport requests\\n\\npd.set_option(\\\"max_colwidth\\\", 80)\";\n",
       "                var nbb_formatted_code = \"from pathlib import Path\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport os\\nimport requests\\n\\npd.set_option(\\\"max_colwidth\\\", 80)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# python program to check if a directory exists\\nDATA_PATH = \\\"data\\\"\\n# Check whether the specified path exists or not\\nisExist = os.path.exists(DATA_PATH)\\nif not isExist:\\n\\n    # Create a new directory because it does not exist\\n    os.makedirs(DATA_PATH)\\n    print(\\\"The new directory is created!\\\")\";\n",
       "                var nbb_formatted_code = \"# python program to check if a directory exists\\nDATA_PATH = \\\"data\\\"\\n# Check whether the specified path exists or not\\nisExist = os.path.exists(DATA_PATH)\\nif not isExist:\\n\\n    # Create a new directory because it does not exist\\n    os.makedirs(DATA_PATH)\\n    print(\\\"The new directory is created!\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# python program to check if a directory exists\n",
    "DATA_PATH = \"data\"\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(DATA_PATH)\n",
    "if not isExist:\n",
    "\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(DATA_PATH)\n",
    "    print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"response = requests.get(\\n    \\\"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_features.csv\\\",\\n)\\n\\nwith open(os.path.join(DATA_PATH, \\\"training_set_features.csv\\\"), \\\"wb\\\") as f:\\n    f.write(response.content)\";\n",
       "                var nbb_formatted_code = \"response = requests.get(\\n    \\\"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_features.csv\\\",\\n)\\n\\nwith open(os.path.join(DATA_PATH, \\\"training_set_features.csv\\\"), \\\"wb\\\") as f:\\n    f.write(response.content)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    \"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_features.csv\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"training_set_features.csv\"), \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"response = requests.get(\\n    \\\"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_labels.csv\\\",\\n)\\n\\nwith open(os.path.join(DATA_PATH, \\\"training_set_labels.csv\\\"), \\\"wb\\\") as f:\\n    f.write(response.content)\";\n",
       "                var nbb_formatted_code = \"response = requests.get(\\n    \\\"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_labels.csv\\\",\\n)\\n\\nwith open(os.path.join(DATA_PATH, \\\"training_set_labels.csv\\\"), \\\"wb\\\") as f:\\n    f.write(response.content)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    \"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/training_set_labels.csv\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"training_set_labels.csv\"), \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"train_metadata = pd.read_csv(\\\"training_set_features.csv\\\")\\ntrain_labels = pd.read_csv(\\\"training_set_labels.csv\\\")\";\n",
       "                var nbb_formatted_code = \"train_metadata = pd.read_csv(\\\"training_set_features.csv\\\")\\ntrain_labels = pd.read_csv(\\\"training_set_labels.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata = pd.read_csv(\"training_set_features.csv\")\n",
    "train_labels = pd.read_csv(\"training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image ID</th>\n",
       "      <th>Storm ID</th>\n",
       "      <th>Relative Time</th>\n",
       "      <th>Ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nhe_000</td>\n",
       "      <td>nhe</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nhe_001</td>\n",
       "      <td>nhe</td>\n",
       "      <td>1800</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nhe_002</td>\n",
       "      <td>nhe</td>\n",
       "      <td>3600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nhe_003</td>\n",
       "      <td>nhe</td>\n",
       "      <td>5402</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nhe_004</td>\n",
       "      <td>nhe</td>\n",
       "      <td>9001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image ID Storm ID  Relative Time  Ocean\n",
       "0  nhe_000      nhe              0      2\n",
       "1  nhe_001      nhe           1800      2\n",
       "2  nhe_002      nhe           3600      2\n",
       "3  nhe_003      nhe           5402      2\n",
       "4  nhe_004      nhe           9001      2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"train_metadata.head()\";\n",
       "                var nbb_formatted_code = \"train_metadata.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70257, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"train_metadata.shape\";\n",
       "                var nbb_formatted_code = \"train_metadata.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consist of images identified by a unique `image_id`. Each `image_id` is composed of `{storm_id}_{image_number}`, where `storm_id` is a unique three letter code and `image_number` represents the sequential ordering of images throughout that storm.\n",
    "\n",
    "Let's take a look at how many storms are in the training data, and how many images we have per storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_ID\n",
      "Storm_ID\n",
      "Relative_Time\n",
      "Ocean\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"for col in train_metadata.columns:\\n    print(col)\";\n",
       "                var nbb_formatted_code = \"for col in train_metadata.columns:\\n    print(col)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in train_metadata.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.nunique of 0        nhe\n",
       "1        nhe\n",
       "2        nhe\n",
       "3        nhe\n",
       "4        nhe\n",
       "        ... \n",
       "70252    yti\n",
       "70253    yti\n",
       "70254    yti\n",
       "70255    yti\n",
       "70256    yti\n",
       "Name: Storm ID, Length: 70257, dtype: object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"train_metadata[\\\"Storm ID\\\"].nunique\\n# \";\n",
       "                var nbb_formatted_code = \"train_metadata[\\\"Storm ID\\\"].nunique\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata[\"Storm ID\"].nunique\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"train_metadata.columns = [c.replace(' ', '_') for c in train_metadata.columns]\";\n",
       "                var nbb_formatted_code = \"train_metadata.columns = [c.replace(\\\" \\\", \\\"_\\\") for c in train_metadata.columns]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.columns = [c.replace(\" \", \"_\") for c in train_metadata.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.nunique of 0        nhe\n",
       "1        nhe\n",
       "2        nhe\n",
       "3        nhe\n",
       "4        nhe\n",
       "        ... \n",
       "70252    yti\n",
       "70253    yti\n",
       "70254    yti\n",
       "70255    yti\n",
       "70256    yti\n",
       "Name: Storm_ID, Length: 70257, dtype: object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"train_metadata.Storm_ID.nunique\";\n",
       "                var nbb_formatted_code = \"train_metadata.Storm_ID.nunique\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.Storm_ID.nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'storm_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m storm_counts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorm_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m      2\u001b[0m storm_counts\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/pandas/core/frame.py:8392\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8390\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8395\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:889\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    887\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'storm_id'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"storm_counts = train_metadata.groupby(\\\"storm_id\\\").size()\\nstorm_counts.describe()\";\n",
       "                var nbb_formatted_code = \"storm_counts = train_metadata.groupby(\\\"storm_id\\\").size()\\nstorm_counts.describe()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "storm_counts = train_metadata.groupby(\"storm_id\").size()\n",
    "storm_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(storm_counts, bins=50, color=\"lightgray\")\n",
    "plt.xlabel(\"Number of Images\")\n",
    "plt.ylabel(\"Number of Storms\")\n",
    "plt.title(\"Number of Images per Storm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contain images from 494 storms. We have anywhere from 4 to 648 images per unique storm, with many storms containing fewer than 100 images.\n",
    "\n",
    "We can also take a look at how many storms come from each ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.groupby(\"ocean\")[\"storm_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's explore the overall distribution of wind speeds in the training data by looking at `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.boxplot(x=train_labels.wind_speed, color=\"lightgray\")\n",
    "plt.xlabel(\"Wind Speed\")\n",
    "plt.title(\"Distribution of Wind Speeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind speeds in the training data range from 15 to 185 knots. A majority of images fall between 30 and 62 knots, but given what we know about storm intensity, it is especially important that we be able to accurately estimate the highest wind speeds.\n",
    "\n",
    "Given the temporal nature of storm and wind patterns, relative time will be an important correlate of wind speed. Our metadata contains a relative time field measured in seconds since the beginning of a storm. We can merge `train_metadata` with `train_labels` to see how wind speeds change over the course of a storm for a couple of examples.\n",
    "\n",
    "**Note: Don't forget that the goal of this competition is to produce an operational model that uses recent images to estimate future wind speeds. Your models may take advantage of the temporal data provided for each storm, up to the point of prediction. It may not use any information from images captured later in a storm to estimate the wind speeds of images captured earlier in a storm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train_metadata with train_labels on the image_id field\n",
    "full_metadata = train_metadata.merge(train_labels, on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wind_speeds(storm_id):\n",
    "    storm = full_metadata[full_metadata.storm_id == storm_id]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.scatter(\"relative_time\", \"wind_speed\", data=storm, color=\"lightgray\")\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.ylabel(\"Wind Speed\")\n",
    "    plt.title(f\"Wind Speed over Relative Time: Storm {storm_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample two random storms from full_metadata\n",
    "for storm in full_metadata.storm_id.sample(2, random_state=40):\n",
    "    plot_wind_speeds(storm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For storm `pvj`, wind speeds begin around 40 knots, steadily increase to 120 knots, and then drop off again. Storm `xhj` displays a similar pattern, except that wind speeds hover around 70 knots before dropping off. While there appear to be short periods of time for which we do not have images available for these two storms, there is a clear relationship between relative time and wind speed that should be incorporated into our model. \n",
    "\n",
    "Keep in mind that the test set contains storms not represented in the training set, so our model needs to be able to generalize to storms it hasn't seen before. For storms represented in both the training and test sets, the test set images always temporally succeed the training set images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can begin exploring the image data. We'll add a `file_name` column to our training metadata, which will contain a `Path` object with the full path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path style access for pandas\n",
    "import pandas_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata[\"file_name\"] = (\n",
    "    DATA_PATH / \"train\" / full_metadata.image_id.path.with_suffix(\".jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at some of the storm imagery!  \n",
    "\n",
    "Remember that because the data are single-band infrared images captured by Geostationary Operational Environmental Satellites (GOES), pixel values represent heat energy in the infrared spectrum. Data for this competition come from band #13. We will be able to see objects, like weather clouds, based on their temperatures, but these representations are actually invisible to the human eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_wind_speed(wind_speed):\n",
    "    sample_img = full_metadata[full_metadata.wind_speed == wind_speed].file_name.iloc[0]\n",
    "    return Image(str(sample_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a relatively low wind speed image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a relatively high wind speed image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! How about the image with the *highest* wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_wind_speed(wind_speed=185)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each increase in wind speed, we can see noticeable changes in storm structure and intensity. An effective model will be able to detect these types of patterns at scale.\n",
    "\n",
    "Finally, let's confirm that the first few training images are the expected size of 366 x 366 pixels using [Pillow](https://pypi.org/project/Pillow/), imported as `PIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    pil_image.open(full_metadata.iloc[i][\"file_name\"]).convert(\"RGB\") for i in range(5)\n",
    "]\n",
    "for image in examples:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set for this competition includes a set of storms not included in the training data, as well as unseen imagery from later in a training storm's life cycle. We do not want to overstate our model's performance by overfitting to one or more storms. To be sure that our method is sufficiently generalizable, we will set aside a portion of the training imagery to validate the model during its development.\n",
    "\n",
    "Since observations in a time series are not independent, we cannot randomly subset the training data into training and validation sets. In other words, the characteristics of temporal data, such as trends and seasonality, require that we take time into account when splitting the data. From a real-world perspective, it will always be the case that we will use images and wind measurements taken earlier in a storm to estimate wind speeds when new imagery comes in.\n",
    "\n",
    "To account for the temporal relationship of the data up to the point of prediction and to ensure that we are not using images captured later in a storm to estimate prior wind speeds, we will subset the training data into training and validation sets using the `relative_time` field. For the purposes of this benchmark, we will holdout the last 20% of each storm's available images for our validation set.\n",
    "\n",
    "You are encouraged to further incorporate relative time into your own model and should consider the implications of autocorrelation when determining how best to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a temporary column for number of images per storm\n",
    "images_per_storm = full_metadata.groupby(\"storm_id\").size().to_frame(\"images_per_storm\")\n",
    "full_metadata = full_metadata.merge(images_per_storm, how=\"left\", on=\"storm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each storm is sorted by relative time\n",
    "# Identify the final 20% of images per storm\n",
    "full_metadata[\"pct_of_storm\"] = (\n",
    "    full_metadata.groupby(\"storm_id\").cumcount() / full_metadata.images_per_storm\n",
    ")\n",
    "train = full_metadata[full_metadata.pct_of_storm < 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")\n",
    "val = full_metadata[full_metadata.pct_of_storm >= 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm pct of images in the validation set is approximately 20%\n",
    "len(val) / len(full_metadata) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this benchmark, we will use a randomly selected subset of 10% of our available data for training and validation. This adjustment will increase performance as we build our initial pipeline, but can be adjusted back for final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% to increase performance\n",
    "train = train.sample(frac=0.1, replace=False, random_state=1)\n",
    "val = val.sample(frac=0.1, replace=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "x_train = train.drop(\"wind_speed\", axis=1)\n",
    "y_train = train.wind_speed\n",
    "\n",
    "x_val = val.drop(\"wind_speed\", axis=1)\n",
    "y_val = val.wind_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our first pass is to build a relatively simple model that outputs wind speed predictions given single-band infrared imagery as input. Once we test this basic approach, we can add additional elements of sophistication and complexity, such as hyperparameter tuning or sequence modeling. We will use a lightweight PyTorch wrapper called [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) for this benchmark solution.\n",
    "\n",
    "Rather than train an entire Convolutional Neural Network (CNN) from scratch, we will [fine-tune](https://cs231n.github.io/transfer-learning/) a pretrained model for transfer learning. This means that we will initialize our weights using a model that has been pretrained on a huge image dataset, replace and retrain its fully connected layer, and update the weights of the entire network by continuing backpropagation. There are many [pretrained models](https://pytorch.org/docs/stable/torchvision/models.html) to choose from. For this exercise, we will use a network called [ResNet 152](https://arxiv.org/abs/1512.03385), which was prepared by Microsoft Research Asia in 2015 for the Large Scale Visual Recognition Challenge and is pretrained on the ImageNet dataset.\n",
    "\n",
    "First, we will need to read the training data into memory, convert the data to PyTorch tensors, and serve the data to our model in batches. Luckily, the PyTorch `Dataset` and `DataLoader` classes make implementing these complex tasks relatively straightforward. A `Dataset` object allows us to define custom methods for working with the data, and a `DataLoader` object parallelizes data loading. If you haven't worked with these classes before, we highly recommend this short [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "Our custom dataset will inherit an abstract class called `torch.utils.data.Dataset` and [override](https://www.tutorialspoint.com/overriding-methods-in-python) the following methods:\n",
    "\n",
    "- `__len__()`: returns the length of the dataset, measured as number of samples\n",
    "- `__getitem__()`: provided an index, returns a sample from the dataset\n",
    "\n",
    "The dataset object will return samples as dictionaries with keys for:\n",
    "\n",
    "- `image_id`: the image id\n",
    "- `image`: the image tensor\n",
    "- `label`: the label, if it exists\n",
    "\n",
    "Since `resnet152` was trained using images smaller than 366 x 366 pixels, we will use the `torchvision.transforms.Compose` module to downsize our images using center cropping, convert them to PyTorch tensors, and normalize their pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWIND(Dataset):\n",
    "    \"\"\"Reads in an image, transforms pixel values, and serves\n",
    "    a dictionary containing the image id, image tensors, and label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train, y_train=None):\n",
    "        self.data = x_train\n",
    "        self.label = y_train\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(128),\n",
    "                transforms.ToTensor(),\n",
    "                # All models expect the same normalization mean & std\n",
    "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = pil_image.open(self.data.iloc[index][\"file_name\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image_id = self.data.iloc[index][\"image_id\"]\n",
    "        if self.label is not None:\n",
    "            label = self.label.iloc[index]\n",
    "            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n",
    "        else:\n",
    "            sample = {\n",
    "                \"image_id\": image_id,\n",
    "                \"image\": image,\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of processing the data, we can use `torch.utils.data.DataLoader` to serve the data within our model class (more on this later).\n",
    "\n",
    "Next, we will create a custom class to define our loss function, Root Mean Square Error (RMSE). As a reminder, RMSE represents the square root of the mean of the squared differences between the predicted values and the actual values. This class will inherit `nn.Module`, which is the [base class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) for all neural network modules in PyTorch. The `forward` method is built upon `MSELoss()`, which measures mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"Measures root mean square error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        return torch.sqrt(self.mse(pred, true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our new `DatasetWIND` and `RMSELoss` classes, PyTorch Lightning will allow us to train a model using minimal for loops. This library keeps the flexibility of PyTorch but removes most of the boilerplate training code, making it less error prone, cleaner to read, and easier to update. By subclassing  `pl.LightningModule`, most of the training logic will happen for us behind the scenes. If you're new to PyTorch Lightning, you may find their [quick start](https://pytorch-lightning.readthedocs.io/en/latest/new-project.html) guide helpful.\n",
    "\n",
    "We will define the following methods within our model:\n",
    "\n",
    "- `prepare_model`: import a pretrained model and reinitialize the final layer with a new sequence of modules\n",
    "- `forward`: define the forward pass for an image\n",
    "- `training_step` (required): switch the model to train mode, implement the forward pass, and calculate training loss for a batch\n",
    "- `validation_step`: switch the model to eval mode and calculate validation loss for a batch\n",
    "- `train_dataloader` (required): call an iterable over the training dataset for automatic batching\n",
    "- `val_dataloader`: call an iterable over the validation dataset for automatic batching\n",
    "- `configure_optimizers` (required): configure an [optimizer](https://pytorch.org/docs/stable/optim.html) (we will use Adam); this step automatically calls `backward` and `step` in each epoch\n",
    "- `training_epoch_end`: calculate the average loss for an epoch with the outputs of `training_step`\n",
    "- `validation_epoch_end`: calculate the average loss for an epoch with the outputs of `validation_step`\n",
    "\n",
    "We'll also add a couple additional, optional, helper methods, e.g., `fit` to instantiate and fit a `pl.Trainer` object for training automation, and `make_submission_frame` to prepare our competition submission csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedWindModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(PretrainedWindModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 2e-4)\n",
    "        self.hidden_size = self.hparams.get(\"embedding_dim\", 50)\n",
    "        self.dropout = self.hparams.get(\"dropout\", 0.1)\n",
    "        self.max_epochs = self.hparams.get(\"max_epochs\", 1)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 0)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 10)\n",
    "        self.x_train = self.hparams.get(\"x_train\")\n",
    "        self.y_train = self.hparams.get(\"y_train\")\n",
    "        self.x_val = self.hparams.get(\"x_val\")\n",
    "        self.y_val = self.hparams.get(\"y_val\")\n",
    "        self.num_outputs = 1  # One prediction for regression\n",
    "\n",
    "        # Where final model will be saved\n",
    "        self.output_path = Path.cwd() / self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Where TensorBoard logs will be saved\n",
    "        self.log_path = Path.cwd() / self.hparams.get(\"log_path\", \"logs\")\n",
    "        self.log_path.mkdir(exist_ok=True)\n",
    "        self.logger = pl.loggers.TensorBoardLogger(\n",
    "            self.log_path, name=\"benchmark_model\"\n",
    "        )\n",
    "\n",
    "        # Instantiate training and validation datasets\n",
    "        self.train_dataset = DatasetWIND(self.x_train, self.y_train)\n",
    "        self.val_dataset = DatasetWIND(self.x_val, self.y_val)\n",
    "        self.model = self.prepare_model()\n",
    "\n",
    "    def prepare_model(self):\n",
    "        res_model = models.resnet152(pretrained=True)\n",
    "        # Input size of 2048 for resnet152\n",
    "        # https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "        res_model.fc = nn.Sequential(\n",
    "            nn.Linear(2048, self.hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_size, self.num_outputs),\n",
    "        )\n",
    "        return res_model\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        criterion = RMSELoss()\n",
    "        # Switch to training mode\n",
    "        loss = criterion(\n",
    "            self.model.train().forward(x).squeeze(), y.type(torch.FloatTensor)\n",
    "        )\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"]\n",
    "        criterion = RMSELoss()\n",
    "        # Switch to evaluation mode\n",
    "        loss = criterion(\n",
    "            self.model.eval().forward(x).squeeze(), y.type(torch.FloatTensor)\n",
    "        )\n",
    "        tensorboard_logs = {\"val_loss\": loss}\n",
    "        return {\"batch_val_loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack(tuple(output[\"loss\"] for output in outputs)).mean()\n",
    "        return {\n",
    "            \"avg_epoch_train_loss\": avg_train_loss,\n",
    "            \"progress_bar\": {\"avg_epoch_train_loss\": avg_train_loss},\n",
    "            \"log\": {\"avg_epoch_train_loss\": avg_train_loss},\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack(\n",
    "            tuple(output[\"batch_val_loss\"] for output in outputs)\n",
    "        ).mean()\n",
    "        return {\n",
    "            \"avg_epoch_val_loss\": avg_val_loss,\n",
    "            \"progress_bar\": {\"avg_epoch_val_loss\": avg_val_loss},\n",
    "            \"log\": {\"avg_epoch_val_loss\": avg_val_loss},\n",
    "        }\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def fit(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            default_root_dir=self.output_path,\n",
    "            logger=self.logger,\n",
    "            checkpoint_callback=pl.callbacks.ModelCheckpoint(\n",
    "                filepath=self.output_path,\n",
    "                monitor=\"avg_epoch_val_loss\",\n",
    "                mode=\"min\",\n",
    "                verbose=True,\n",
    "            ),\n",
    "            gradient_clip_val=self.hparams.get(\"gradient_clip_val\", 1),\n",
    "            num_sanity_val_steps=self.hparams.get(\"val_sanity_checks\", 0),\n",
    "        )\n",
    "        self.trainer.fit(self)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, x_test):\n",
    "        test_dataset = DatasetWIND(x_test)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, num_workers=self.num_workers, batch_size=self.batch_size\n",
    "        )\n",
    "        submission_frame = pd.DataFrame(index=x_test.image_id, columns=[\"wind_speed\"])\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            x = batch[\"image\"]\n",
    "            preds = self.eval().forward(x)\n",
    "            submission_frame.loc[batch[\"image_id\"], \"wind_speed\"] = (\n",
    "                preds.detach().numpy().squeeze()\n",
    "            )\n",
    "        submission_frame.wind_speed = submission_frame.wind_speed.astype(float)\n",
    "        return submission_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to fit our model. A `PretrainedWindModel` can be instantiated using only a dictionary of `hparams`. The only required `hparams` are the training and validation data, but there are several additional hyperparameters we can specify to explore modeling strategies, including the learning rate, dropout rate, and hidden layer size. Consider experimenting with different models and using hyperparameter tuning to find the best combination of parameters to increase performance.\n",
    "\n",
    "Once we specify our `hparams`, we can simply call the `fit` method to begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # Required hparams\n",
    "    \"x_train\": x_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"x_val\": x_val,\n",
    "    \"y_val\": y_val,\n",
    "    # Optional hparams\n",
    "    \"lr\": 2e-4,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_epochs\": 4,\n",
    "    \"batch_size\": 10,\n",
    "    \"num_workers\": 0,\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"val_sanity_checks\": 0,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"log_path\": \"logs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_model = PretrainedWindModel(hparams=hparams)\n",
    "storm_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: PyTorch Lightning lets you log PyTorch models and metrics into a directory for visualization within the [TensorBoard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) UI. TensorBoard is a machine learning visualization toolkit that's helpful for tracking metrics across batches, epochs, and models. You can install TensorBoard through the command line to visualize your logged data by specifying the root log directory and running `tensorboard --logdir=logs/benchmark_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we are finally ready to perform inference and make a submission. You'll only want to perform inference on the test set once you determine your top performing model, to avoid inadvertently overfitting.\n",
    "\n",
    "First, let's take a look at the submission format. Make sure to that your submission sets `image_id` as the index to avoid getting a submission error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(\n",
    "    DATA_PATH / \"submission_format.csv\", index_col=\"image_id\"\n",
    ")\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightning automatically saves everything you need to restore training sessions as [checkpoints](https://pytorch-lightning.readthedocs.io/en/latest/weights_loading.html) in your current working directory. We updated this directory using the `output_path` hyperparameter in `hparams`. Even though model training has the potential to get interrupted, you should always be able to pick back up where you left off without having to restart training from scratch.\n",
    "\n",
    "Let's load our best model from the `model-outputs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best checkpoint based on logs\n",
    "best_checkpoint = str(Path(\"model-outputs\") / \"epoch=3.ckpt\")\n",
    "example_model = PretrainedWindModel.load_from_checkpoint(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can estimate wind speeds in the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    \"https://radiant-mlhub.s3-us-west-2.amazonaws.com/nasa-tropical-storm-challenge/test_set_features.csv\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"test_set_features.csv\"), \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = pd.read_csv(DATA_PATH / \"test_set_features.csv\")\n",
    "test_metadata[\"file_name\"] = (\n",
    "    DATA_PATH / \"test\" / test_metadata.image_id.path.with_suffix(\".jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = example_model.make_submission_frame(test_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure submission indices match submission format\n",
    "assert submission_format.index.equals(submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few predictions look reasonable, based on our training data exploration. However, `wind_speed` must be an integer based on the `submission_format`, so let's round our final predictions to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.round().astype(int)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the range of wind speeds that we are estimating in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.wind_speed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimum wind speed of 24 knots and a maximum wind speed of 153 knots seems like a reasonable range.\n",
    "\n",
    "Finally, we will save our submission locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv((DATA_PATH / \"submission.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now head to the competition [submissions page](https://www.drivendata.org/competitions/72/predict-wind-speeds/submissions/) and upload our submission to obtain our model's RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drivendata-public-assets.s3.amazonaws.com/re-benchmark-score.png\" width=\"700\" height=\"60\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see an RMSE of 12.6912 on the leaderboard. That's a great start, but now it's up to you to make improvements and lower this error! We hope this benchmark solution provides a helpful framework for exploring the data, designing a machine learning pipeline, and training a deep learning model to predict the wind speeds associated with storm imagery for disaster preparedness.\n",
    "\n",
    "Head over to the [Wind-dependent Variables](https://www.drivendata.org/competitions/72/predict-wind-speeds/) challenge homepage to get started. We can't wait to see what you build!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
