{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# How to use the Radiant MLHub API to browse and download the NASA Tropical Storm Wind Speed Competition Data\n",
    "\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the NASA Tropical Storm Wind Speed Competition dataset. Full documentation for the API is available at [docs.mlhub.earth](http://docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization,retrieve the items (the data contained within them) from those collections, and load the data into a dataframe.\n",
    "\n",
    "Each item in our collection is explained in json format compliant with STAC label extension definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "M. Maskey, R. Ramachandran, I. Gurung, B. Freitag, M. Ramasubramanian, J. Miller (2020) \"Tropical Cyclone Wind Estimation Competition Dataset\", Version 1.0, Radiant MLHub. \\[Date Accessed\\] [https://doi.org/10.34911/rdnt.xs53up](https://doi.org/10.34911/rdnt.xs53up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This notebook utilizes the [`radiant-mlhub` Python client](https://pypi.org/project/radiant-mlhub/) for interacting with the API and the [`pandas`](https://pandas.pydata.org/) library for compiling the data. If you are running this notebooks using Binder, then these dependency has already been installed. If you are running this notebook locally, you will need to install this yourself.\n",
    "\n",
    "See the official [`radiant-mlhub` docs](https://radiant-mlhub.readthedocs.io/) for more documentation of the full functionality of that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "### Create an API Key\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [mlhub.earth/profile](https://mlhub.earth/profile). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key(s), which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details). \n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "*If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = 'PASTE_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from radiant_mlhub import Dataset, Collection, client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Collections\n",
    "\n",
    "A Radiant MLHub *Dataset* is a group of related *Collections*. We can use the `Dataset.list` method to get a list of the available datasets as Python objects and inspect their `id` and `title` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umd_mali_crop_type: (2019 Mali CropType Training Data)\n",
      "idiv_asia_crop_type: (A crop type dataset for consistent land cover classification in Central Asia)\n",
      "dlr_fusion_competition_germany: (A Fusion Dataset for Crop Type Classification in Germany)\n",
      "ref_fusion_competition_south_africa: (A Fusion Dataset for Crop Type Classification in Western Cape, South Africa)\n",
      "ref_agrifieldnet_competition_v1: (AgriFieldNet Competition Dataset)\n",
      "bigearthnet_v1: (BigEarthNet)\n",
      "microsoft_chesapeake: (Chesapeake Land Cover)\n",
      "c2smsfloods_v1: (Cloud to Street - Microsoft flood dataset)\n",
      "csu_synthetic_attribution: (CSU Synthetic Attribution Benchmark Dataset)\n",
      "ref_african_crops_kenya_02: (CV4A Kenya Crop Type Competition)\n",
      "ref_african_crops_uganda_01: (Dalberg Data Insights Crop Type Uganda)\n",
      "rti_rwanda_crop_type: (Drone Imagery Classification Training Dataset for Crop Types in Rwanda)\n",
      "cgiar_east_africa_agricultural_field_centers: (East Africa Agricultural Field Centers)\n",
      "lacuna_fund_eotg_v1: (Eyes on the Ground Image Data)\n",
      "ref_african_crops_tanzania_01: (Great African Food Company Crop Type Tanzania)\n",
      "ref_landcovernet_af_v1: (LandCoverNet Africa)\n",
      "ref_landcovernet_as_v1: (LandCoverNet Asia)\n",
      "ref_landcovernet_au_v1: (LandCoverNet Australia)\n",
      "ref_landcovernet_eu_v1: (LandCoverNet Europe)\n",
      "ref_landcovernet_na_v1: (LandCoverNet North America)\n",
      "ref_landcovernet_sa_v1: (LandCoverNet South America)\n",
      "marida_v1: (Marine Debris Archive (MARIDA))\n",
      "nasa_marine_debris: (Marine Debris Dataset for Object Detection in Planetscope Imagery)\n",
      "nasa_floods_v1: (NASA Flood Extent Detection)\n",
      "open_cities_ai_challenge: (Open Cities AI Challenge Dataset)\n",
      "ref_african_crops_kenya_01: (PlantVillage Crop Type Kenya)\n",
      "ramp_accra_ghana: (ramp Building Footprint Training Dataset - Accra, Ghana)\n",
      "ramp_barishal_bangladesh: (ramp Building Footprint Training Dataset - Barishal, Bangladesh)\n",
      "ramp_bentiu_south_sudan: (ramp Building Footprint Training Dataset - Bentiu, South Sudan)\n",
      "ramp_chittagong_bangladesh: (ramp Building Footprint Training Dataset - Chittagong, Bangladesh)\n",
      "ramp_coxs_bazar_bangladesh: (ramp Building Footprint Training Dataset - Cox's Bazar, Bangladesh)\n",
      "ramp_daressalaam_tanzania: (ramp Building Footprint Training Dataset - Dar es Salaam, Tanzania)\n",
      "ramp_dhaka_bangladesh: (ramp Building Footprint Training Dataset - Dhaka, Bangladesh)\n",
      "ramp_hpa_an_myanmar: (ramp Building Footprint Training Dataset - Hpa-an, Myanmar)\n",
      "ramp_jashore_bangladesh: (ramp Building Footprint Training Dataset - Jashore, Bangladesh)\n",
      "ramp_karnataka_india: (ramp Building Footprint Training Dataset - Karnataka, India)\n",
      "ramp_les_cayes_haiti: (ramp Building Footprint Training Dataset - Les Cayes, Haiti)\n",
      "ramp_lubumbashi_drc: (ramp Building Footprint Training Dataset - Lubumbashi, Democratic Republic of the Congo)\n",
      "ramp_manjama_sierra_leone: (ramp Building Footprint Training Dataset - Manjama, Sierra Leone)\n",
      "ramp_mesopotamia_st_vincent: (ramp Building Footprint Training Dataset - Mesopotamia, St. Vincent)\n",
      "ramp_muscat_oman: (ramp Building Footprint Training Dataset - Muscat, Oman)\n",
      "ramp_mzuzu_malawi: (ramp Building Footprint Training Dataset - Mzuzu, Malawi)\n",
      "ramp_nairobi_kenya: (ramp Building Footprint Training Dataset - Nairobi, Kenya)\n",
      "ramp_ndjamena_chad: (ramp Building Footprint Training Dataset - N'Djamena, Chad)\n",
      "ramp_paris_france: (ramp Building Footprint Training Dataset - Paris, France)\n",
      "ramp_shanghai_china: (ramp Building Footprint Training Dataset - Shanghai, China)\n",
      "ramp_sylhet_bangladesh: (ramp Building Footprint Training Dataset - Sylhet, Bangladesh)\n",
      "ramp_wa_ghana: (ramp Building Footprint Training Dataset - Wa, Ghana)\n",
      "nasa_rwanda_field_boundary_competition: (Rwanda Field Boundary Competition Dataset)\n",
      "su_african_crops_ghana: (Semantic Segmentation of Crop Type in Ghana)\n",
      "su_african_crops_south_sudan: (Semantic Segmentation of Crop Type in South Sudan)\n",
      "sen12floods: (SEN12-FLOOD : A SAR and Multispectral Dataset for Flood Detection)\n",
      "sen12ts_v1: (SEN12TS: A SAR and Multispectral Dataset for Land Cover Classification)\n",
      "ref_cloud_cover_detection_challenge_v1: (Sentinel-2 Cloud Cover Segmentation Dataset)\n",
      "ts_cashew_benin: (Smallholder Cashew Plantations in Benin)\n",
      "ref_south_africa_crops_competition_v1: (South Africa Crop Type Competition)\n",
      "spacenet1: (SpaceNet 1)\n",
      "spacenet2: (SpaceNet 2)\n",
      "spacenet3: (SpaceNet 3)\n",
      "spacenet4: (SpaceNet 4)\n",
      "spacenet5: (SpaceNet 5)\n",
      "spacenet6: (SpaceNet 6)\n",
      "spacenet7: (SpaceNet 7)\n",
      "nasa_tropical_storm_competition: (Tropical Cyclone Wind Estimation Competition)\n",
      "su_sar_moisture_content_main: (Western USA Live Fuel Moisture)\n"
     ]
    }
   ],
   "source": [
    "for dataset in Dataset.list():\n",
    "    print(f'{dataset.id}: ({dataset.title})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in the \"Tropical Cyclone Wind Estimation Competition\" dataset. We can fetch this dataset using its\n",
    "ID (`nasa_tropical_storm_competition`) and then use the `collections` property to list the source imagery and label collections associated with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unarchive nasa_tropical_storm_competition.tar.gz: 100%|█| 458546/458546 [00:56<00:00, 8175.18\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.fetch('nasa_tropical_storm_competition')\n",
    "dataset.collection_descriptions\n",
    "\n",
    "dataset.download(catalog_only=True)\n",
    "# print('Source Imagery Collections\\n--------------------------')\n",
    "# for collection in dataset.:\n",
    "#     print(collection.list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Imagery Collections\n",
      "--------------------------\n",
      "nasa_tropical_storm_competition_train_source\n",
      "nasa_tropical_storm_competition_test_source\n",
      "\n",
      "Label Collections\n",
      "-----------------\n",
      "nasa_tropical_storm_competition_train_labels\n",
      "nasa_tropical_storm_competition_test_labels\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.fetch('nasa_tropical_storm_competition')\n",
    "\n",
    "print('Source Imagery Collections\\n--------------------------')\n",
    "for collection in dataset.collections.source_imagery:\n",
    "    print(collection.id)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Label Collections\\n-----------------')\n",
    "for collection in dataset.collections.labels:\n",
    "    print(collection.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset has 2 collections containing source imagery for this dataset and 1 collection containing \n",
    "labels.\n",
    "\n",
    "The following cell gets the first item from each collection and prints the item ID, as well as a summary of the assets associated with the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: nasa_tropical_storm_competition_train_source\n",
      "Item: nasa_tropical_storm_competition_train_source_zzp_225\n",
      "Assets:\n",
      "- image: Hurricane Image [image/jpeg]\n",
      "- features: Image Features [application/json]\n",
      "\n",
      "\n",
      "Collection: nasa_tropical_storm_competition_test_source\n",
      "Item: nasa_tropical_storm_competition_test_source_zza_178\n",
      "Assets:\n",
      "- image: Hurricane Image [image/jpeg]\n",
      "- features: Image Features [application/json]\n",
      "\n",
      "\n",
      "Collection: nasa_tropical_storm_competition_train_labels\n",
      "Item: nasa_tropical_storm_competition_train_labels_zzp_225\n",
      "Assets:\n",
      "- labels: Wind Speed Label [application/json]\n",
      "\n",
      "\n",
      "Collection: nasa_tropical_storm_competition_test_labels\n",
      "Item: nasa_tropical_storm_competition_test_labels_zza_178\n",
      "Assets:\n",
      "- labels: Wind Speed Label [application/json]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_summary(item, collection):\n",
    "    print(f'Collection: {collection.id}')\n",
    "    print(f'Item: {item[\"id\"]}')\n",
    "    print('Assets:')\n",
    "    for asset_name, asset in item.get('assets', {}).items():\n",
    "        print(f'- {asset_name}: {asset[\"title\"]} [{asset[\"type\"]}]')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "for collection in dataset.collections:\n",
    "    item = next(client.list_collection_items(collection.id, limit=1))\n",
    "    print_summary(item, collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items in the  `*train_labels` collection have a `\"labels\"` JSON asset containing wind speed labels for each source image. Items in the `*test_source` and `*train_source` collections have both a `\"features\"` JSON asset containing image features as JSON and an `\"image\"` JPEG asset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Assets\n",
    "\n",
    "In the following section, we download all JSON assets for both the `test` and `train` collections. ML Hub makes archives available that contain all the assets for a given collection. We will download these archives for the `nasa_tropical_storm_competition_train_labels` and `nasa_tropical_storm_competition_test_source` collections and then extract the items that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download to a data folder the current working directory\n",
    "# download_dir = Path('./data').resolve()\n",
    "\n",
    "# # Use this to download the the typical Mac user Downloads folder\n",
    "download_dir = Path('~/Downloads').expanduser().resolve()\n",
    "\n",
    "# # Use this to download to the typical Linux /tmp directory\n",
    "# download_dir = Path('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import radiant_mlhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radiant_mlhub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/michelleroby/Downloads')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unarchive nasa_tropical_storm_competition.tar.gz: 100%|█| 458546/458546 [00:13<00:00, 33244.1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/radiant_mlhub/models/dataset.py:361\u001b[0m, in \u001b[0;36mDataset.download\u001b[0;34m(self, output_dir, catalog_only, if_exists, api_key, profile, bbox, intersects, datetime, collection_filter)\u001b[0m\n\u001b[1;32m    347\u001b[0m config \u001b[38;5;241m=\u001b[39m CatalogDownloaderConfig(\n\u001b[1;32m    348\u001b[0m     catalog_only\u001b[38;5;241m=\u001b[39mcatalog_only,\n\u001b[1;32m    349\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     temporal_query\u001b[38;5;241m=\u001b[39mdatetime,\n\u001b[1;32m    359\u001b[0m )\n\u001b[1;32m    360\u001b[0m dl \u001b[38;5;241m=\u001b[39m CatalogDownloader(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m--> 361\u001b[0m \u001b[43mdl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/radiant_mlhub/client/catalog_downloader.py:740\u001b[0m, in \u001b[0;36mCatalogDownloader.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# call each step\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[0;32m--> 740\u001b[0m     \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# inspect the error report\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merr_report\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/radiant_mlhub/client/catalog_downloader.py:282\u001b[0m, in \u001b[0;36mCatalogDownloader._create_asset_list_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m             _handle_collection(stac_item)\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m             \u001b[43m_handle_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstac_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_unfiltered_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique assets in stac catalog.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/radiant-earth/mlhub-tutorials/WIND/lib/python3.10/site-packages/radiant_mlhub/client/catalog_downloader.py:223\u001b[0m, in \u001b[0;36mCatalogDownloader._create_asset_list_step.<locals>._handle_item\u001b[0;34m(stac_item)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_item\u001b[39m(stac_item: JsonDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     item_id \u001b[38;5;241m=\u001b[39m \u001b[43mstac_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    224\u001b[0m     assets \u001b[38;5;241m=\u001b[39m stac_item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    225\u001b[0m     props \u001b[38;5;241m=\u001b[39m stac_item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "dataset.download(output_dir=download_dir)\n",
    "\n",
    "print('Done\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into a Dataframe\n",
    "\n",
    "The cells below will load both the training and test items into dataframes, join the two, and sort the rows by the Image ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "train_source = 'nasa_tropical_storm_competition_train_source'\n",
    "train_labels = 'nasa_tropical_storm_competition_train_labels'\n",
    "\n",
    "jpg_names = glob(str(download_dir / train_source / '**' / '*.json'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "    \n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "    labels_path = str(jpg_path.parent / 'labels.json').replace(train_source, train_labels)\n",
    "\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "        \n",
    "    # Load the labels data\n",
    "    with open(labels_path) as src:\n",
    "        labels_data = json.load(src)\n",
    "\n",
    "    train_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "        int(labels_data['wind_speed'])\n",
    "    ])\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    np.array(train_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean', 'Wind Speed']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "test_source = 'nasa_tropical_storm_competition_test_source'\n",
    "\n",
    "jpg_names = glob(str(download_dir / test_source / '**' / '*.jpg'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "\n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "\n",
    "    test_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "    ])\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.array(test_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
